\clearpage

# Introduction

The aim of this project was to be able to take any song and make an accurate prediction about it's "Beat's Per Minute" (BPM), the standard measure of the tempo of a song.  No external signal processing libraries were used in this project, and instead everything was built from scratch, in order to learn more about signal processing.  The only library used was the WebAudio API, in order to convert various audio formats into an array of floating point numbers representing the raw signal.  

The project was implemented as a webpage, and is available for your viewing pleasure here: [http://www.cse.unsw.edu.au/~lukegt/bpm/]().  It works best in Google Chrome.  Just drag and drop a song into the indicated area and wait a few seconds while it churns through the mathematics.  The interface may become unresponsive while this happens, but soon the song will begin playing and the output of the program will display.  If you are interested (and game enough) to look at the code itself, you can view it here

The general approach was to first find the envelope of the signal using the Hilbert transform, approximated as a Finite Impulse Response filter.  The envelope was then passed through a Fast Fourier Transform algorithm to find the frequencies of the key beats in the song.  Then a heuristic driven algorithm was used to find the best match for the primary beat and its harmonics, taking into account a variety of possible time signatures.  The output of the algorithm is a BPM measurement, an estimation of the song's time signature, a characteristic beat wave to represent the varying strengths of beats, the location of each beat in the signal, and the location of the beginning of each bar.  

# Methodology

We will now take a look at how the algorithm works in more detail.  Not everything about the algorithm is explained here - only the interesting things.  

## Downsampling

Modern music is stored at a very high quality, with a 5 minute song containing 13.23 million samples if it is recorded at the standard rate of 44100Hz.  It is unpractical to analyse so many samples, especially when we don't need to, since the information about the tempo of the song is present at a very low frequency.  

As a result, the first step of the algorithm is to down-sample the audio signal.  To simplify things, the signal was down-sampled at an integer ratio, which meant that we could simply select a subset of samples spread out at regular intervals throughout the signal.  

This process results in what is called aliasing, causing new frequencies to appear in the down-sampled signal that were not present in the original.  The reason for this is to do with Shannon's Sampling Theorem, which states that "the sampling frequency should be twice the maximal frequency present in the signal".  This means that lowering the sampling rate also lowers the highest frequencies we can represent, causing the high frequency waves from the original signal to simply introduce noise.  See Figure \ref{fig:aliasing} for a visual explanation.  

\begin{figure}
    \centering
    \includegraphics{images/downsample_aliasing.png}
    \caption{Here we can see how sampling a high frequency wave (red) at an insufficient sample rate can cause other undesired frequencies (blue) to appear.}
    \label{fig:aliasing}
\end{figure}

Normally this issue is resolved by running the original signal through a low pass filter, removing the frequencies which would cause aliasing.  However, for the purposes of finding the beat of a song, the effects of aliasing are minimal, and so this issue was not dealt with in this project.  

The amount by which the original audio signal was down-sampled by was determined by how many samples the following parts of the algorithm could handle, largely bottle-necked by the fast Fourier transform.  The signal was down-sampled as much as possible while still retaining the required amount of samples for analysis, a number which is discussed later.  So the down-sample ratio is given by $r$ if we need $S$ many samples and the signal contains $N$ many samples:

\begin{align}
r = \left\lfloor \frac{N}{S} \right\rfloor
\end{align}

Down-sampling would involve taking every $r^{th}$ sample from the original signal.

## Hilbert Transform

The Hilbert transform is a linear operator which takes in a signal $u$ and produces the harmonic conjugate of $u$.  The Hilbert transform is key in deriving the *analytic signal* of a signal $u$, which is a complex representation of a signal that makes interesting properties more readily available.  It removes all the negative frequencies from the signal, which are redundant and unnecessary.  If $H$ is the Hilbert transform operator, and $A$ is the analytic signal of $u$, we get:

\begin{align}
A &= u + iH[u]
\end{align}

The analytic signal allows us to easily find the envelope $E$ of the signal $u$ at a point in time $t$ by simply taking the modulus of the analytic signal:

\begin{align}
E(t) &= | A(t) | \notag\\
E(t) &= | u(t) + iH[u](t) | \notag\\
E(t) &= \sqrt{ u(t)^2 + H[u](t)^2 }
\end{align}

The envelope of a signal is a representation of the signal's amplitude over time.  As a result the envelope is key in determining the tempo of a song, since a beat is essentially a periodic change in amplitude.  

The Hilbert transform can be defined as a simple multiplier operator on the fourier transform of a signal.  If we denote the Fourier transform by $\mathcal{F}$, then its definition is as follows:

\begin{align}
\mathcal{F}[H[u]](\omega) = (-i\ sgn(\omega)) \cdot \mathcal{F}[u](\omega)
\end{align}

So this will essentially multiply all negative frequencies by $i$, and all positive frequencies by $-i$.  Since $i = e^{ i \frac{\pi}{2} }$ and $-i = e^{ -i\frac{\pi}{2} }$, this essentially results in a phase shift of $\frac{\pi}{2}$ for each of the negative frequencies and $-\frac{\pi}{2}$ for positive frequencies.  

This definition is the same for when we use the discrete Fourier transform $\mathcal{DF}$.  We can define the function $h$, where $\mathcal{DF}[h](\omega) = -i\ sgn(\omega)$, then use convolution theorem to represent the above dot product as a convolution instead, and then take the inverse discrete Fourier transform of both sides to give:

\begin{align}
\mathcal{DF}[H[u]](\omega) &= \mathcal{DF}[h](\omega) \cdot \mathcal{DF}[u](\omega) \notag\\
\mathcal{DF}[H[u]] &= \mathcal{DF}[h * u] \notag\\
H[u] &= h * u
\end{align}

So we can find the Hilbert transform of a signal $u$ by simply convolving it with a response function $h$, given by^[I had a shot at deriving this, but I couldn't crack it in the space of time that I had. This result is taken from the Wikipedia article on the Hilbert transform]:

\begin{align}
    h(n) &= \mathcal{DF}^{-1}[-i\ sgn](n) \notag\\
    h(n) &= 
    \begin{cases}
        0 & \text{for even $n$} \\
        \frac{2}{\pi n} & \text{for odd $n$}
    \end{cases}
\end{align}

However, calculating this convolution for a very large amount of samples is not feasible, since for $N$ samples we must perform $N^2$ multiplications.  Instead we approximate the response function as a Finite Impulse Response (FIR), which means that we choose a constant $w$ such that all values of $h(n)$ further than $w$ away from the origin are simply 0, and do not need to be considered.

\begin{align}
    h(n) &= 
    \begin{cases}
        0 & \text{for even $n$ or if $|n| > w$} \\
        \frac{2}{\pi n} & \text{otherwise}
    \end{cases}
\end{align}

Now if we have $N$ samples, we must only perform $Nw$ multiplications, which is much more feasible if we choose a small $w$.  However, the smaller the value of $w$, the less accurate the FIR approximation will be.  

The actual $w$ chosen for this project was very low at 8.  This was chosen empirically by observing the final spectrum of the envelope.  It appeared that increasing the quality of the Hilbert transform had little effect on the quality of the spectrum in the frequencies of interest, so a very small value was chosen in order to speed up the algorithm^[I'm aware that I could have calculated the Hilbert transform by applying FFT, shifting the phases appropriately, then applying inverse FFT, but when I realised this I didn't have enough time to implement it. That being said, the current implementation works quite quickly, and using an FFT would definitely slow things down.].  

Once the Hilbert transform of the signal was found, it was combined with the original signal to give the analytic signal, and the modulus of this signal was found to get the envelope.  

## Fast Fourier Transform

Once the envelope function is calculated, we need to perform a discrete Fourier transform on it to find the primary beat, which will appear as a frequency in the spectrum of the envelope.  The discrete Fourier transform of a signal $u$ of length $N$ is given by

\begin{align}
    \mathcal{DF}[u](k) = \frac{1}{\sqrt{N}} \sum\limits_{n=0}^{N-1} u(n) e^{-i\frac{2\pi}{N}nk}
\end{align}

Clearly this is not feasible to compute for a large $N$ since it will require $N^2$ many operations to compute all values of $u$.  Thankfully this is a well studied problem, and there are many known techniques which can be used to perform a *fast Fourier transform*, which achieves the desired result in a feasible time.  The most well-known technique is known as the Cooley-Tukey FFT algorithm.  We will observe the most simple version, the RADIX-2 decimation in time algorithm.  It works recursively, breaking the above sum down into two sums, one containing the even terms and one containing the odd terms.  Note that it only works for when $N$ is a power of 2. 

\begin{align}
    \mathcal{DF}[u](k) &= \frac{1}{\sqrt{N}} \sum\limits_{n=0}^{\frac{1}{2}N-1} u(2n) e^{-i\frac{2\pi}{N}(2n)k} + \frac{1}{\sqrt{N}} \sum\limits_{n=0}^{\frac{1}{2}N-1} u(2n+1) e^{-i\frac{2\pi}{N}(2n+1)k} \notag\\
    \mathcal{DF}[u](k) &= \frac{1}{\sqrt{N}} \sum\limits_{n=0}^{\frac{1}{2}N-1} u_{even}(n) e^{-i\frac{2\pi}{\frac{1}{2}N}nk} + \frac{1}{\sqrt{N}} e^{-i\frac{2\pi}{N}k}\sum\limits_{n=0}^{\frac{1}{2}N-1} u_{odd}(n) e^{-i\frac{2\pi}{\frac{1}{2}N}nk} \notag\\
    \mathcal{DF}[u](k) &= \frac{1}{\sqrt{2}} \mathcal{DF}[u_{even}](k) + \frac{1}{\sqrt{2}} e^{-i\frac{2\pi}{N}k}\ \mathcal{DF}[u_{odd}](k) \label{eq:fft-recursion-basic}
\end{align}

Hence we can solve the problem recursively, requiring only $k$ multiplications per recursive call (ignoring the normalisations by $\frac{1}{\sqrt{2}}$, which can be replaced by a single multiplication of $\frac{1}{\sqrt{N}}$ at the end of computation).  This in itself will provide a speed boost, giving an $O(N\ log\ N)$ time complexity, but there are some other tricks we can employ to further reduce computation time.  

Firstly, we can reduce the amount of multiplications performed to $\frac{1}{2}k$ by observing that the "twiddle" term $e^{-i\frac{2\pi}{N}k}$ is simply the $k^{th}$ roots of unity, and that one half of the roots of unity are simply the negatives of the other half:

\begin{align}
    e^{-i\frac{2\pi}{N}(k + \frac{1}{2}N)} &= e^{-i\frac{2\pi}{N}k} e^{-i\frac{2\pi}{N}\frac{1}{2}N} \notag\\
    &= e^{-i\frac{2\pi}{N}k} e^{-i\pi} \notag\\
    &= - e^{-i\frac{2\pi}{N}k}
\end{align}

This means that we can split equation (\ref{eq:fft-recursion-basic}) into two, for all $0 \le k < \frac{1}{2}N$:

\begin{align}
    \mathcal{DF}[u](k) &= \mathcal{DF}[u_{even}](k) + e^{-i\frac{2\pi}{N}k}\ \mathcal{DF}[u_{odd}](k)\\
    \mathcal{DF}[u](k + \frac{1}{2}N) &= \mathcal{DF}[u_{even}](k) - e^{-i\frac{2\pi}{N}k}\ \mathcal{DF}[u_{odd}](k)
\end{align}

The multiplication performed in both of these equations is actually the same, and so we have successfully halved the amount of multiplications necessary at each step.  

Note that we also dodged a bullet here in that the length of $u_{even}$ and $u_{odd}$ is $\frac{1}{2}N$, and we were previously addressing values beyond their range in equation (\ref{eq:fft-recursion-basic}).  It wouldn't have been a problem since both are periodic functions, but it's worth mentioning.  

Performance can further be improved by pre-computing the "twiddle" values, as they re-appear at deeper stages of the recursion.  We only need to compute values for where $0 \le k < \frac{1}{2}N$, and they can be stored in an array indexed by $k$.  For an execution at recursive depth $d$, the twiddle value for $k$ is given by $k2^d$, since $N$ halves as we recurse downwards.  

Below is the pseudo-code for the FFT algorithm used in this project, assuming that we have the twiddle values cached in an array called `twiddle`, and that our samples are stored in the array `u`.  Instead of slicing and dicing the signal `u` into odd and even lists, we define what subset of the original signal `u` we are referring to, starting from the element indexed by `first`, and taking `N` samples `step` indicies apart from one another. 

    function fft(u, first, N, step)
        if N is 1
            result[0] = u[first]
        else
            even = fft(u, first, N/2, step*2)
            odd  = fft(u, first + step, N/2, step*2)
            for k in [0, N/2)
                multiplication = twiddle[k*step] * odd[k]
                result[k] = even[k] + multiplication
                result[k + N/2] = even[k] - multiplication
        return result

Using this algorithm, a signal with $2^20 = 1048576$ samples was able to undergo a discrete Fourier transformation in a reasonable amount of time (less than 2 seconds on my computer).  

## Finding the best beat

Once we have the spectrum of beat frequencies $U = \mathcal{DF}[u]$, one might think it is a simple task to find the frequency which corresponds to the tempo of the song by finding the value $\omega$ which maximises $|U(\omega)|$.  Unfortunately things are not so easy, as many different frequencies will contain components of the main beat in some way, and will sometimes dominate the magnitude of the primary beat.  

This is expressed visually in Figures \ref{fig:4/4} and \ref{fig:3/4}.  We can see in these figures that if the primary beat has a frequency of $\omega$, we will get a number of equally spaced frequencies lower than $\omega$ that will also appear strongly in our spectrum.  These waves are harmonics of the lowest frequency wave among them.  

This can however work to our advantage.  There is a tendency for music to have some beats which are more prominent than others, for example the majority of songs will see every $4^{th}$ beat played more strongly.  This tendency is described by the *time signature* of a song, which is made up of two numbers, e.g. $\begin{smallmatrix} 4\\4 \end{smallmatrix}$.  The top number states how many beats are in a *bar*, and the bottom number states what kind of note represents a beat on sheet music (not relevant to us, we will assume that this is 4 always).  A *bar* is a collection of beats, the first of which is the most prominent.  The important part here is that the set of harmonic amplitude waves described above will often converge on the first beat of each bar.  For example, in Figure \ref{fig:4/4}, every $4^{th}$ beat sees 3 to 4 waves having a maximum at that point in time.  The waves in Figure \ref{fig:3/4} see 2 to 3 waves having a maximum at every $3^{rd}$ beat.  

\begin{figure}
\centering

    \begin{tikzpicture}
    \begin{axis}[
        domain = 0:3,
        xtick = {0, 1, ..., 3},
        width = \textwidth,
        height = 0.2\textheight
    ]
        \addplot [red,    no markers, samples = 256] {
            0.5 + 0.5 * cos(deg( 2*pi * x * 4/4 ))};
        \addplot [orange, no markers, samples = 256] {
            0.5 + 0.5 * cos(deg( 2*pi * x * 4/3))};
        \addplot [green!80!black,  no markers, samples = 256] {
            0.5 + 0.5 * cos(deg( 2*pi * x * 4/2))};
        \addplot [blue, very thick, no markers, samples = 256] {
            0.5 + 0.5 * cos(deg( 2*pi * x * 4/1))};
    \end{axis}
    \end{tikzpicture}

\caption[]{
    Here we see the various amplitude waves we might see in a song with a
    $\begin{smallmatrix} 4 \\ 4 \end{smallmatrix}$
    time signature.  The blue wave is the wave which corresponds to the tempo of the song.  The green orange and red waves are at $\frac{3}{4}$, $\frac{2}{4}$ and $\frac{1}{4}$ of the frequency of the blue wave respectively.  
}
\label{fig:4/4}

\end{figure}


\begin{figure}
\centering

    \begin{tikzpicture}
    \begin{axis}[
        domain = 0:4,
        xtick = {0, 1, ..., 4},
        width = \textwidth,
        height = 0.2\textheight
    ]
        \addplot [orange, no markers, samples = 256] {
            0.5 + 0.5 * cos(deg( 2*pi * x * 3/3 ))};
        \addplot [green!80!black, no markers, samples = 256] {
            0.5 + 0.5 * cos(deg( 2*pi * x * 3/2 ))};
        \addplot [blue, very thick, no markers, samples = 256] {
            0.5 + 0.5 * cos(deg( 2*pi * x * 3/1 ))};
    \end{axis}
    \end{tikzpicture}

\caption[]{
    Here we see the various amplitude waves we might see in a song with a
    $\begin{smallmatrix} 3 \\ 4 \end{smallmatrix}$
    time signature.  The blue wave is the wave which corresponds to the tempo of the song.  The green and orange waves are at $\frac{2}{3}$ and $\frac{1}{3}$ of the frequency of the blue wave respectively.  
}
\label{fig:3/4}

\end{figure}

This means that if we have a song with a time signature of $\begin{smallmatrix} n\\4 \end{smallmatrix}$, then not only will the correct tempo frequency $\omega$ be prominent in the spectrum, but the frequencies $\frac{1}{n}\omega$, $\frac{2}{n}\omega$, ..., and $\frac{n-1}{n}\omega$ will also be prominent.  These will be referred to as *component frequencies*.  

Our search for the tempo frequency can be guided by this property.  Our task is to find a frequency $\omega$ which has large values for all its $n$ component frequencies.  Since there is only a small number of tempos and time signatures which are feasible, our search space is very small.  As a result the approach chosen was to iterate over all possible time signatures $n$, and for each time signature iterate over all possible frequencies $\omega$.  The best $\omega$ for each $n$ was determined using one kind of scoring function, and the best $\omega$ overall was determined using another.  This process has the nice property that in addition to a prediction about the BPM of the song, we get a prediction for its time signature.  

The minimum and maximum tempos allowed were empirically determined based on testing, fixed at 30BPM and 200BPM respectively.  The minimum and maximum time signatures allowed were also empirically determined, fixed at $\begin{smallmatrix} 3 \\ 4 \end{smallmatrix}$ and $\begin{smallmatrix} 23 \\ 4 \end{smallmatrix}$ respectively.  

The scoring function used to pick the best frequency $\omega$ for a given $n$ was simply the sum of the magnitudes of each frequency in the spectrum $U$:

\begin{align}
s_1(\omega) &= \sum\limits_{k = 1}^{n} |U(\frac{k}{n} \omega)|
\end{align}

The scoring function used to pick the best frequency $\omega$ overall was to pick the one which had the largest magnitude at frequency $2\omega$, the beat's first harmonic, in addition to having a large magnitude itself.  If $\omega$ was the correct beat, it would be very likely that $2\omega$ would have a large response, since it has a peak at every beat.  

\begin{align}
s_2(\omega) &= |U(\omega)| + |U(2\omega)|
\end{align}

Although these scoring functions were simple, they did a very good job of picking the correct tempo frequency among a noisy spectrum.  See Figure \ref{fig:beat-spectrum} for an example of a correct selection among much noise.  

## Characteristic Beat Wave

The characteristic beat wave, as it will be referred to, is the wave which is formed as a result of combining all the component waves of the tempo.  The beats in each bar of a song will have varying strengths, with many $\begin{smallmatrix} 4\\4 \end{smallmatrix}$ songs having a beat pattern of strong-weak-medium-weak.  The characteristic beat wave attempts to illustrate the strength pattern of beats in a bar.  It is this wave that animated the coloured balls on the project's web page, which appear to "dance" in time to the music.  

We find this wave by filtering the beat spectrum $U$, only keeping values which were a part of the component frequencies of the beat, setting everything else to 0.  We then perform an inverse discrete Fourier transform to get the wave that these frequencies create.  

An inverse discrete Fourier transform is almost exactly the same as the forwards discrete Fourier transform.  The only difference is that we need to negate time.  

\begin{align}
    \mathcal{DF}[u](k) = \frac{1}{\sqrt{N}} \sum\limits_{n=0}^{N-1} u(n) e^{i\frac{2\pi}{N}nk}
\end{align}

So instead of re-implementing the fast Fourier transform algorithm with this one minor adjustment, we can use a trick to make the original algorithm behave as the inverse.  

If we swap the imaginary and real parts of the input, feed this into our FFT, then swap the imaginary and real parts of the output, this will be equivalent to the inverse discrete Fourier transform.  This is simple to implement as well, since it is just a matter of swapping pointers.  The exact reason as to why this works is not explained here in detail, but it is to do with the fact that swapping the real and imaginary parts of a complex number is the same as finding its complex conjugate and multiplying by $i$.  The mixture of this conjugation and this multiplication by $i$ results in effectively negating the power in the roots of unity.  

Interestingly, when the beat wave was calculated, it contained both a real and imaginary part, both which described and responded to different aspects of the beat.  In the project's web page, the blue ball follows the real part of the beat wave, and the green ball follows the imaginary part.  By adding these two parts together, aspects of both were merged to form something which was more representative of the beat as a whole.  The red ball moves with this addition of the real and imaginary parts.  

## Finding the position of the beat

Finding the frequency of a beat is one thing, but to know when it occurs in the signal is another.  Thankfully there is enough information in the discrete Fourier transform to determine this.  In fact, we also have enough information to make a prediction about where each bar begins.  

The argument of the beat spectrum $U$ gives us the phase of each frequency, which essentially tells us in radians the angle at which each wave begins at.  By observing the tempo frequency $\omega$'s phase value, we can determine where the first beat occurs, and then easily determine where each following beat occurs.  If the phase is 0, there is a beat at the very start of the signal, whereas if it is some value $\phi$, the first beat occurs at a position corresponding to an angle of $2\pi - \phi$ in the wave.  i.e., if it has a frequency $\omega$, then the first beat occurs at

\begin{align}
    t = \frac{2\pi - \phi}{2\pi\omega}
\end{align}

If a song has the strongest beat as the first beat of every bar, then we can find the beginning of the first bar by performing the same analysis on the bottom component frequency, $\frac{\omega}{n}$.

Calculations like these are used in the working project to produce the clock, which ticks on each beat, ideally aiming to hit 12 o'clock on the first beat of a bar.  

# Results

The algorithm was tested on 32 challenging songs, all of varying tempo, time signature and beat complexity.  Note that because these songs were chosen in a challenging way, these pie charts do not represent the algorithm's performance in the average case.  Popular music with a strong $\begin{smallmatrix} 4\\4 \end{smallmatrix}$ beat was almost always predicted correctly.  Figures \ref{fig:bpm-accuracy}, \ref{fig:time-signature-accuracy}, \ref{fig:beat-phase-accuracy} and \ref{fig:bar-phase-accuracy} show pie charts demonstrating how the algorithm shaped up to my own brain's beat detection algorithm (which was taken as ground truth).  

It is worth noting that it is very easy to get the tempo wrong by an integer factor, so the instances where the algorithm did this are shown within the pie charts.  To demonstrate how difficult it is to see which frequencies correspond to the beat, observe Figure \ref{fig:beat-spectrum}.  The correctly identified component frequencies are marked with red lines, but there are several other spikes surrounding that which appear prominently also.  The feature that makes the primary (rightmost) beat frequency stick out from the rest is its combination of a strong set of component frequencies, and a very strong first harmonic.  

It was positive to see that the algorithm could largely correctly identify the time signature, even though it was given a huge selection to choose from.  The difference between a $\begin{smallmatrix} 4\\4 \end{smallmatrix}$ song and a $\begin{smallmatrix} 3\\4 \end{smallmatrix}$ song was largely distinguished.  In particular, a strange song with a $\begin{smallmatrix} 22\\4 \end{smallmatrix}$ time signature was put forward, and the algorithm correctly identified its time signature and BPM.  

It is also worth noting that in most instances where the algorithm got the BPM incorrect, forcing it to use the correct time signature often resulted in the correct result.  This implies that the algorithm used in selecting the time signature may need improvement.  

\begin{figure}
\includegraphics{images/bpm-accuracy.png}
\caption{}
\label{fig:bpm-accuracy}
\end{figure}

\begin{figure}
\includegraphics{images/time-signature-accuracy.png}
\caption{}
\label{fig:time-signature-accuracy}
\end{figure}

\begin{figure}
\includegraphics{images/beat-phase-accuracy.png}
\caption{}
\label{fig:beat-phase-accuracy}
\end{figure}

\begin{figure}
\includegraphics{images/bar-phase-accuracy.png}
\caption{}
\label{fig:bar-phase-accuracy}
\end{figure}

\begin{figure}
\includegraphics{images/beat-spectrum.png}
\caption{}
\label{fig:beat-spectrum}
\end{figure}